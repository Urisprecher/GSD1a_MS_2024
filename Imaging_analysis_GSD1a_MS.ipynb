{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pptx import Presentation \n",
    "from pptx.util import Inches \n",
    "import seaborn as sns \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import scipy\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.offline import iplot\n",
    "import cufflinks as cf\n",
    "from plotly import __version__ \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45904dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df, group_col):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame and a column to group by as inputs and returns\n",
    "    a summary DataFrame with statistical parameters for each group.\n",
    "    \"\"\"\n",
    "    # Group the DataFrame by the specified column\n",
    "    grouped_df = df.groupby(group_col)\n",
    "   \n",
    "    # Define a dictionary to hold the statistical parameters to compute for each column\n",
    "    agg_dict = {}\n",
    "   \n",
    "    # Loop over the columns in the DataFrame and add the statistical parameters to the agg_dict\n",
    "    for col in df.columns:\n",
    "        if col != group_col:\n",
    "            agg_dict[col] = [\n",
    "                ('count', 'count'),\n",
    "                ('mean', 'mean'),\n",
    "                ('std', 'std'),\n",
    "                ('min', 'min'),\n",
    "                ('25%', lambda x: np.quantile(x, 0.25)),\n",
    "                ('median', 'median'),\n",
    "                ('75%', lambda x: np.quantile(x, 0.75)),\n",
    "                ('max', 'max')\n",
    "                \n",
    "            ]\n",
    "   \n",
    "    # Compute the summary statistics for each group and column using the agg_dict\n",
    "    summary_df = grouped_df.agg(agg_dict)\n",
    "   \n",
    "    # Flatten the multi-index column names into a single level\n",
    "    summary_df.columns = [f'{col}_{stat}' for col, stat in summary_df.columns]\n",
    "   \n",
    "    # Rename the index column\n",
    "    summary_df.index.name = group_col\n",
    "   \n",
    "    # Return the summary DataFrame\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9affb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_pre(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            #Nh_df_24 = Nh_df_24.drop(columns=[\"PC\", \"group_with_pc\", \"group\", \"group_with_id\"])\n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of log({col})\")\n",
    "            plt.xlabel(f\"log({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(curr_out_path + '//' + f\"hist_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"Q-Q_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"dist_pre{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19777c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check_if_max(frame):\n",
    "    \n",
    "    print(\"frame_ready\")\n",
    "    for col in frame:\n",
    "            #Nh_df_24 = Nh_df_24.drop(columns=[\"PC\", \"group_with_pc\", \"group\", \"group_with_id\"])\n",
    "            plt.hist(frame[col])\n",
    "            plt.title(f\"Histogram of log({col})\")\n",
    "            plt.xlabel(f\"log({col})\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            plt.savefig(curr_out_path + '//' + f\"hist_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"hist_done!\")\n",
    "            plt.figure()\n",
    "            scipy.stats.probplot(frame[col], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"Q-Q_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"qq_done:)\")\n",
    "            plt.figure()\n",
    "            sns.distplot(frame[col], kde = True, color ='red', bins = 30)\n",
    "            plt.title(f\"dist-{col}\")\n",
    "            plt.show\n",
    "            plt.savefig(curr_out_path + '//' + f\"dist_max{col}.pdf\", dpi = 600)\n",
    "            plt.close()\n",
    "            print(\"on it:)\")\n",
    "            print(\"done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ba8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff607a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bdc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PAS ASSAY ANALYSIS\n",
    "path = ('Results_PAS') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')\n",
    "input_path = [f for f in glob.glob(os.path.join('PAS_DATA', '*.csv'))]\n",
    "print(input_path) \n",
    "verbose = False\n",
    "for path_i in input_path: \n",
    "    CLEAN_df = pd.read_csv(path_i, skip_blank_lines=True)\n",
    "    path_i.split('.csv')[0].split('PAS_DATA')[1]\n",
    "    name = path_i.split('.csv')[0].split('PAS_DATA\\\\')[1]\n",
    "    print('Current file name {}'.format(name))\n",
    "    curr_out_path = path + '//' + '{}'.format(name)\n",
    "    if not os.path.exists(curr_out_path):\n",
    "        os.mkdir(curr_out_path)\n",
    "        print('Output folder created')  \n",
    "    ##CLEAN AND INDEX\n",
    "    CLEAN_df = CLEAN_df.drop(columns=['Plate ID', 'Row', 'Column'])\n",
    "    CLEAN_df[\"group\"] = CLEAN_df[\"Cell Type\"].astype(str) + CLEAN_df[\"Compound\"].astype(str)\n",
    "    CLEAN_df.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "    CLEAN_df[\"group_with_id\"] = + CLEAN_df[\"Cell Type\"].astype(str) + CLEAN_df[\"Compound\"].astype(str) + CLEAN_df[\"CELL ID\"].astype(str)\n",
    "    CLEAN_df.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "    #drop NA\n",
    "    CLEAN_df.dropna(axis=0, thresh=30)\n",
    "    #remove unwanted cells in analysis \n",
    "    # clear low and high cell count\n",
    "    cell_count =  CLEAN_df[\"Nuclei Nuclei Count wv1\"].values\n",
    "    CLEAN_df = CLEAN_df.loc[cell_count<1200]\n",
    "    cell_count =  CLEAN_df[\"Nuclei Nuclei Count wv1\"].values\n",
    "    CLEAN_df = CLEAN_df.loc[cell_count>100]\n",
    "    ##EXPORT FOR STATITICS AND VIS\n",
    "    df_clean = CLEAN_df.copy()\n",
    "    df_clean.to_csv(curr_out_path + '//' + '{}_forstat.csv'.format(name))\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF per MARKER\n",
    "path = ('Results_IF_markerx') \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print('Output folder created')\n",
    "input_path = [f for f in glob.glob(os.path.join('folder', '*.csv'))]\n",
    "print(input_path) \n",
    "verbose = False\n",
    "for path_i in input_path: \n",
    "    CLEAN_df = pd.read_csv(path_i, skip_blank_lines=True)\n",
    "    path_i.split('.csv')[0].split('folder')[1]\n",
    "    name = path_i.split('.csv')[0].split('folder\\\\')[1]\n",
    "    print('Current file name {}'.format(name))\n",
    "    curr_out_path = path + '//' + '{}'.format(name)\n",
    "    if not os.path.exists(curr_out_path):\n",
    "        os.mkdir(curr_out_path)\n",
    "        print('Output folder created')  \n",
    "    # clean and index\n",
    "    CLEAN_df = CLEAN_df.drop(columns=['Number of Analyzed Fields', 'Time [s]', 'Row', 'Column',\n",
    "                           'Concentration', 'Cell Count', 'Plane', 'Timepoint'])\n",
    "    CLEAN_df[\"group\"] = CLEAN_df[\"Cell Type\"].astype(str) + CLEAN_df[\"Compound\"].astype(str)\n",
    "    CLEAN_df.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "    CLEAN_df[\"group_with_id\"] = + CLEAN_df[\"Cell Type\"].astype(str) + CLEAN_df[\"Compound\"].astype(str) + CLEAN_df[\"CELL ID\"].astype(str)\n",
    "    CLEAN_df.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "    #drop NA\n",
    "    CLEAN_df.dropna(axis=0, thresh=30)\n",
    "    # clear low and high cell count\n",
    "    cell_count =  CLEAN_df[\"all_cells - Number of Objects\"].values\n",
    "    CLEAN_df = CLEAN_df.loc[cell_count<1200]\n",
    "    cell_count =  CLEAN_df[\"all_cells - Number of Objects\"].values\n",
    "    CLEAN_df = CLEAN_df.loc[cell_count>100]\n",
    "    print(f'Total number of cells {len(CLEAN_df)}')\n",
    "    ## examine data \n",
    "    df_clean = CLEAN_df.copy()\n",
    "    sum_data_marker = summarize_dataframe(df_clean, \"group\")\n",
    "    file1 = 'marker-summary.csv'\n",
    "    sum_data_marker.to_csv(file1)\n",
    "    df_vis = df_clean[['all_cells - chanel_3_Intensity Mean - Mean per Well']]\n",
    "    dist_check_if_pre(df_vis)\n",
    "    df_vis.plot(kind = 'bar')\n",
    "    print('done')\n",
    "    df_max_scaled = df_clean[['all_cells - chanel_3_Intensity Mean - Mean per Well']]\n",
    "    for column in df_max_scaled.columns:\n",
    "        df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()\n",
    "    df_vis2 = df_max_scaled[['all_cells - chanel_3_Intensity Mean - Mean per Well']]\n",
    "    dist_check_if_max(df_vis2)\n",
    "    df_vis2.plot(kind = 'bar')\n",
    "    print('done')\n",
    "    df_max_scaled.to_csv(curr_out_path + '//' + '{}_max_forstat.csv'.format(name))\n",
    "    print('max_ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mitophagy\n",
    "all_files = glob.glob('mitophagy data/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc828499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index\n",
    "data[\"group\"] = data[\"Cell Type\"].astype(str) + data[\"Compound\"].astype(str)\n",
    "data.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "data[\"group_with_id\"] =  data[\"Cell Type\"].astype(str) + data[\"Compound\"].astype(str) + data[\"CELL ID\"].astype(str)\n",
    "data.set_index([\"group_with_id\"], inplace = True,\n",
    "                            append = True, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac007df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##clear cell count\n",
    "data = data.loc[cell_count<1200]\n",
    "sns.histplot(data[\"all_cells - Number of Objects\"].values)\n",
    "plt.show()\n",
    "cell_count =  data[\"all_cells - Number of Objects\"].values\n",
    "data = data.loc[cell_count>100]\n",
    "sns.histplot(data[\"all_cells - Number of Objects\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = N_df.drop(columns=['CELL ID','Cell Type', 'Column',\n",
    "       'Compound', 'Height [µm]','Number of Analyzed Fields', 'Plane', 'Row', 'Time [s]', 'Timepoint',\n",
    "       'all_cells - Number of Objects', 'group', 'group_with_id'\n",
    "                                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "871fb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_final.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3409690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final frame\n",
    "d = data_final[[\"group\", \"Image Region chanel_2 - chanel_2_in_chanel_2_intensity Mean - Mean per Well\",\n",
    "           \"Image Region chanel_3 - chanel_3_in_chanel_3_intensity Mean - Mean per Well\"]]\n",
    "df_clean_final = d.rename(\n",
    "     columns={ \"Image Region chanel_2 - chanel_2_in_chanel_2_intensity Mean - Mean per Well\": \"m_cherry\",\n",
    "              \"Image Region chanel_3 - chanel_3_in_chanel_3_intensity Mean - Mean per Well\": \"GFP\"})\n",
    "df_clean_final[\"total\"] = df_clean_final[\"m_cherry\"] + df_clean_final[\"GFP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b53eecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_final[\"mitophagy\"] = df_clean_final[\"m_cherry\"] / df_clean_final[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22b8bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index and normalize\n",
    "df_clean_final.set_index([\"group\"], inplace = True,\n",
    "                            append = True, drop = False)\n",
    "hc_frame = df_clean_final[(df_clean_final['group'] == 'HCFCCP')]\n",
    "hc_mean = hc_frame.mean()\n",
    "N_df = df_clean_final/hc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5fe7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export for data vis and stats\n",
    "file_mitophagy = 'mito_proccesed.csv'\n",
    "N_df.to_csv(file_mitophagy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eff9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
